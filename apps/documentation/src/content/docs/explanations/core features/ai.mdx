---
title: AI
description: AI explanation
---

# ‚ùì How it works

This brick is heavily opinionated. It combines Langfuse for prompts management (storage, versionning, combining, etc) and Vercel AI SDK for the LLMs calls.

## Tracing Architecture - OpenTelemetry

The project uses a **unified OpenTelemetry tracing architecture** that integrates both Sentry and Langfuse through a shared TracerProvider. This provides:

- **Distributed Tracing**: Traces are automatically propagated across services, allowing you to see the full request flow from API calls through AI generation
- **Dual Export**: LLM spans are exported to both Langfuse (for prompt management) and Sentry (for full request context)
- **Automatic Instrumentation**: All AI calls are automatically traced without manual code

The tracing system is initialized in `apps/api/src/instrument.ts` before the NestJS application starts. The shared TracerProvider uses:
- **LangfuseSpanProcessor**: Filters and exports only LLM-related spans (instrumentation scope: `langfuse-sdk` or `ai`) to Langfuse
- **SentrySpanProcessor**: Receives all spans for application monitoring in Sentry

**Important**: Because of the shared TracerProvider, Langfuse traces will also appear in Sentry, allowing you to see AI calls in the context of full request traces.

## Prompt management - Langfuse

Langfuse uses OpenTelemetry to trace the calls to the LLMs (it plays well with Vercel SDK). The tracing setup uses a shared TracerProvider that integrates with Sentry. You can see how OpenTelemetry is setup in the [instrument file](https://github.com/lonestone/lonestone-boilerplate/blob/main/apps/api/src/instrument.ts) and how it's used in the [AI service file](https://github.com/lonestone/lonestone-boilerplate/blob/main/apps/api/src/modules/ai/ai.service.ts).

Langfuse can be self hosted, and is entirely open source. See [their docs](https://langfuse.com/docs) for more advanced usage.

## LLM calls - Vercel AI SDK

The Vercel SDK allows us to easily call the LLMs, and to use Langfuse for tracing. It is also quite easy to switch from one model to another.

## Organizing Traces

You can organize AI calls into meaningful traces using the `telemetry` option:

- **`traceName`**: Groups multiple LLM calls with the same `traceName` into a single trace. A deterministic `traceId` is automatically generated from this value.
- **`functionId`**: Identifies the specific function/operation within a trace.
- **`langfuseOriginalPrompt`**: Preserves the original prompt before enhancements (useful for debugging).

See the [AI Module documentation](/lonestone-boilerplate/addons/ai-module#organizing-traces-with-telemetry-options) for detailed examples and usage patterns.

## Orchestration - TBD

The boilerplate does not use any orchestration tool. You can use your own, like Mastra or XState.

# üìù How to use

- Create the Langfuse project in your Langfuse dashboard
- Setup the necessary env variables (LANGFUSE_API_KEY, LANGFUSE_SECRET_KEY, OPENAI_API_KEY, etc) in the `api/.env` file.
- You are good to go!

# üßπ How to remove

1. Delete the `ai` module from the `api/src/modules` folder
2. Remove the following dependencies from the `api/package.json` file:

```json
"@ai-sdk/openai",
"@opentelemetry/api",
"@opentelemetry/auto-instrumentations-node",
"@opentelemetry/sdk-node",
"ai",
"langfuse",
"langfuse-vercel",
```

3. Remove the following env variables from the `api/.env` file:

```
LANGFUSE_API_KEY=
LANGFUSE_SECRET_KEY=
OPENAI_API_KEY=
```
