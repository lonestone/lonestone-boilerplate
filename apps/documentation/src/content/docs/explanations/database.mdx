---
title: Database Migrations
description: Understanding database migrations with MikroORM
---

## MikroORM & TypeScript Entities

[MikroORM](https://mikro-orm.io/) is a TypeScript ORM (Object-Relational Mapping) that lets you define your database schema using TypeScript classes called "entities" instead of writing SQL directly.

Instead of writing SQL like:
```sql
CREATE TABLE users (
  id SERIAL PRIMARY KEY,
  email VARCHAR(255) NOT NULL,
  name VARCHAR(255)
);
```

You write a TypeScript class:
```typescript
@Entity()
export class User {
  @PrimaryKey()
  id!: number

  @Property()
  email!: string

  @Property()
  name?: string
}
```

MikroORM handles translating your TypeScript entities into database tables, columns, and relationships. This approach gives you type safety, IDE autocomplete, and keeps your database schema in sync with your code.

:::note
For more details on MikroORM features and concepts, refer to the [official MikroORM documentation](https://mikro-orm.io/docs).
:::

## Evolving Your Schema Over Time

As your application grows, your data model evolves. You add new fields to your entities, create new tables, modify relationships, or change column types. Since you're working with TypeScript entities, making these changes is straightforward - you simply modify your entity classes.

**In the early stages of development**, this is often enough. You modify your entities, drop and recreate your database from scratch, and keep iterating. MikroORM provides options to reset the entire database schema based on your current entities. There's no formal tracking of changes because the schema is still fluid and you don't have important data to preserve.

**But at some point, this informal approach becomes problematic:**

- You have data you don't want to lose (even in development)
- Multiple developers need to sync schema changes across their local databases
- You're deploying to staging or production environments with real data that must be preserved
- You want a clear history of what changed and when
- You need to be able to review schema changes before they're applied

**When does the "drop and recreate" approach stop being viable?**

The key trigger is **data preservation**. Once you care about keeping existing data, you can't drop and recreate the database anymore. This happens:
- When you start having test data you want to keep locally
- When deploying to staging or production with real user data
- When multiple developers need to share a database with seed data

Other factors that push toward a more structured approach:
- You want traceability of schema changes (who changed what, when, and why)
- You need to review schema changes before they're applied (especially in production)

Some teams use the "drop and recreate" approach for the first few weeks while the schema is highly experimental, then adopt a more formal process once data becomes important. Others use a structured approach from day one for better discipline.

**This structured approach is called "database migrations"** (or "schema migrations"). Instead of recreating everything from scratch, you create versioned files called migrations that describe **incremental changes** to your schema. Each migration represents a single, specific evolution - adding a column, creating a table, modifying a constraint, etc. These migrations can be applied safely to databases that already contain data, preserving the existing records while updating the structure.

## How Migrations Work with MikroORM

MikroORM (like many modern ORMs) provides a built-in system to automatically generate and apply migrations based on changes to your TypeScript entities. This system relies on three elements that work together:

**The snapshot file (`.snapshot.json`)**: A local JSON file that represents the current schema state. Used to detect what changed when generating new migrations.

**Migration files**: Timestamped files containing SQL statements to modify the database. Each file has an `up()` method (apply changes) and a `down()` method (revert changes).

**The migrations table (`mikro_orm_migrations`)**: A database table that tracks which migration files have already been executed in a specific database.

These three elements serve different purposes in the migration lifecycle:
- The **snapshot** helps MikroORM detect what changed in your entities and generate the appropriate SQL
- The **migration files** contain the actual SQL instructions to transform the database schema
- The **migrations table** tracks which migrations have been applied to avoid running them twice

### The Snapshot File

The snapshot is a JSON file located by default in `apps/api/src/modules/db/migrations/.snapshot.json`. It contains a complete representation of your database schema - all entities, their properties, column types, constraints, indexes, and relationships.

**How it works:**

MikroORM provides CLI tools to generate migrations. When you run the migration generation command:

1. MikroORM scans all your TypeScript entities and builds a representation of the desired schema
2. It checks if a `.snapshot.json` file exists:
   - **If no snapshot exists yet** (first migration): MikroORM creates the snapshot file and generates a migration containing SQL to create all your tables from scratch
   - **If a snapshot exists**: MikroORM loads it (representing the previous schema state) and compares it with your current entities
3. It calculates the difference between the current entities and the snapshot (the "diff")
4. It generates a [migration file](#migration-files) with SQL statements for those changes
5. **It immediately updates the snapshot** to reflect the new schema state (including the changes that will be applied)

This approach means you can generate migrations without a database connection - MikroORM compares entities against the snapshot file, not against an actual database.

:::caution[Important]
The snapshot file must be committed to Git along with migration files. If someone generates a migration but doesn't commit the updated snapshot, the next person will see duplicate changes when generating their migration.
:::

### Migration Files

As mentioned above, when you run the MikroORM CLI migration generation command, it automatically creates migration files based on the detected changes. These migration files are TypeScript files with timestamped names (e.g., `Migration20250106143052.ts`) stored in your migrations folder. Each file represents a specific change to your database schema and contains two methods:

**`up()` method**: Contains SQL statements that will be executed directly on your database when you apply the migration. This is the "forward" path that updates your schema to the new state.

```typescript
async up(): Promise<void> {
  this.addSql('ALTER TABLE "user" ADD COLUMN "phone_number" VARCHAR(255);');
}
```

**`down()` method**: Contains SQL statements that will be executed directly on your database when you rollback the migration. This is the "reverse" path that undoes the changes.

```typescript
async down(): Promise<void> {
  this.addSql('ALTER TABLE "user" DROP COLUMN "phone_number";');
}
```

When you run the migration apply command (CLI), MikroORM executes the `up()` SQL against your database. When you run the rollback command, it executes the `down()` SQL.

:::caution[Always review generated SQL]
MikroORM generates these migration files automatically, but **you should always review the SQL before applying it**, especially for production. The generated SQL might not be optimal or safe in all cases:
- Renaming fields: MikroORM might generate DROP + ADD instead of RENAME, losing your data
- Adding NOT NULL columns to tables with existing data will fail
- Complex type changes might need manual data transformation

**You can (and should) manually edit the SQL in migration files when needed.** Just be careful - this SQL will execute directly on your database, so test your changes thoroughly.
:::

**The timestamp naming**

The filename timestamp ensures migrations run in the correct chronological order. When MikroORM applies pending migrations, it sorts them by timestamp and executes them sequentially. This prevents race conditions where migrations are applied in the wrong order across different environments.

MikroORM CLI also allows you to add descriptive names to your migrations (e.g., `Migration20250106143052_add_phone_number_to_user.ts`). This makes it easier to understand what each migration does without opening the file, while still maintaining chronological ordering via the timestamp prefix.

**Example folder structure:**

```
apps/api/src/modules/db/migrations/
├── .snapshot.json                           # Current schema state
├── Migration20250101120000.ts               # First migration (oldest)
├── Migration20250103140530.ts               # Second migration
├── Migration20250105093045.ts               # Third migration
└── Migration20250106143052.ts               # Latest migration (newest)
```

The migrations will always be applied in chronological order: 20250101 → 20250103 → 20250105 → 20250106. The snapshot file reflects the schema state after all these migrations are applied.

### The Migrations Table

We've seen how changes are tracked in your codebase (the snapshot file and migration files), but there's a crucial question: **how does MikroORM know which migrations have already been applied to a specific database?**

A developer might have migrations 1 to 10 in their code, but their local database might only have 1 to 7 applied. Staging might have 1 to 8, and production might only have 1 to 5. Each environment needs to track its own state independently.

This is where the **migrations table** comes in. The `mikro_orm_migrations` table is created automatically in your database when you run your first migration. It tracks which migration files have been executed in that specific database.

**How it works:**

When you run the MikroORM CLI command to apply migrations, it targets the database configured in your current environment (the database connection configured in your application). MikroORM then:

1. Looks at all migration files in your `migrations` folder
2. Checks the `mikro_orm_migrations` table in that database to see which migrations are already applied
3. Identifies all pending migrations (files that exist but aren't recorded in the table)
4. **Automatically executes ALL pending migrations** by running their `up()` methods in chronological order
5. After each successful execution, records that migration in the table

:::note[All pending migrations are applied at once]
The default behavior is to apply **all pending migrations** in a single command execution, not one by one.

For example, if your codebase has migrations 1 to 10, but your database has only applied migrations 1 to 7, running the apply command will automatically execute migrations 8, 9, and 10 in chronological order. Each environment independently tracks and applies only the migrations it's missing.
:::

## Production Safety

In development or staging environments, making mistakes with migrations is relatively low-risk. You can drop your database, regenerate everything, or restore from a recent backup. The consequences are limited.

**But production is fundamentally different.** Production contains real user data, and you obviously don't want to risk losing or corrupting it. A schema change mistake can have much more serious consequences and can be stressful when applying changes to production. That's why there are best practices we'll detail here to protect against these risks.

### Never Drop and Recreate in Production

This might seem obvious, but let's be explicit: dropping and recreating the schema destroys all data. In production environments, you must always use the migration-based workflow.

### Always Backup Before Migrating

Before running any migration in production:

1. Take a full database backup
2. Verify the backup is valid and can be restored
3. **Store the backup in a secure location** with appropriate access controls and encryption
4. Document what time the backup was taken and set a retention policy

If the migration fails or causes issues, you can restore from this backup. Some teams automate this by requiring their deployment scripts to create a backup before running migrations.

:::caution[Data protection]
Production backups contain real user data. Ensure they are:
- **Encrypted** when stored and when transferred over the network
- **Access-controlled** (only authorized personnel can access them)
- **Retained for a limited time** according to your data retention policy
- **Never copied to development or staging environments** without proper anonymization

For non-production environments, always use anonymized or synthetic data, never production backups.
:::

### Validate Migrations in CI/CD Before Production

Before deploying migrations to production, it's a good idea to catch potential issues as early as possible. Running migrations in your CI/CD pipeline, alongside your test suite, helps prevent broken migrations from reaching production.

The typical flow is: migrations are applied first, then your test suite runs against that updated schema. This catches problems before they impact real users:

- SQL syntax errors in migrations
- Breaking changes that cause application errors or test failures
- Schema mismatches between your entities and the actual database structure

### Common Pitfalls with Existing Data

When your database is empty or only has test data, almost any schema change works fine. But once you have real data in production, certain types of changes become problematic because you need to preserve that data while modifying the structure.

Here are common scenarios that require extra attention:

**Adding a required (NOT NULL) field**

If you add a `NOT NULL` column to a table that already has rows, the database will reject the migration - those existing rows don't have a value for this new column. You need to either:
- Add the column as nullable first, fill it with default values, then add the NOT NULL constraint in a second migration
- Add the column with a default value directly

**Renaming a field**

When you rename a field in your entity, MikroORM generates SQL to drop the old column and create a new one. This destroys the data in that column. You need to manually edit the generated migration to use `ALTER TABLE ... RENAME COLUMN` instead, which preserves the data.

**Changing column types**

If you change a field from `string` to `number` in your entity, the generated SQL might just change the column type. But if your existing data can't be converted (like text that isn't a number), the migration will fail. You might need a multi-step migration that transforms the data first.

**The key takeaway:** MikroORM generates SQL based on comparing your entities to the snapshot, but it doesn't understand your actual data. Always review the generated migration and adjust it to handle existing data safely.

:::tip[Automatic rollback on failure]
MikroORM has an `allOrNothing` option (enabled by default in this boilerplate) that wraps each migration in a database transaction. If a migration fails, the entire transaction automatically rolls back, leaving your database in its previous state.

This works for most schema changes (adding/dropping columns, creating tables, etc.), but not for operations that can't be rolled back by the database (like `DROP DATABASE`) or for data transformations outside the migration transaction.
:::

### What to Do When Migrations Fail in Production

Unfortunately, even with careful planning, migrations can sometimes fail in production. When this happens, take a deep breath and follow this checklist:

1. **Don't panic and don't run more commands blindly**
2. Check the migration table (`mikro_orm_migrations`) to see if the migration was marked as executed
3. Check what state your database is actually in (which tables/columns exist)
4. If the migration was marked as executed but didn't complete, you may need to manually mark it as not executed and fix the issue
5. If data was corrupted, restore from your backup
6. Fix the migration file and try again

### Rollback Considerations

Remember that each migration file has both `up()` and `down()` methods. It might be tempting to think "if a migration causes problems in production, I'll just rollback using the `down()` method." However, this is rarely a good solution in production.

In development, rollback is useful - you can apply a migration, realize it's wrong, roll it back, and try again. But **down migrations have serious limitations** that make them problematic in production:

- **Data loss**: If you drop a column, rolling back recreates the column but the data is gone forever
- **Irreversible transformations**: If you transform data (like converting emails to lowercase), rollback can't restore the original values
- **Complex changes**: Some operations (dropping constraints, certain column type changes) can't be perfectly reversed

**In production, rollback is rarely the right solution.** If a migration causes problems, it's usually safer to:
1. Restore from your backup if data was lost or corrupted
2. Write a new forward migration that fixes the issue
3. Only use rollback for immediate issues caught right after deployment, and only if no data was lost

The `down()` method is primarily useful for development workflows, not production recovery.

:::tip[Ready for practical steps?]
Now that you understand how migrations work conceptually, check out the [Migrations Guide](/guides/migrations) for step-by-step instructions on creating, applying, and managing migrations in this boilerplate.
:::