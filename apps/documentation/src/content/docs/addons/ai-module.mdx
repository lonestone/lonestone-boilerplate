---
title: AI Module
description: Guide to using the AI module for text generation with multiple providers, schema validation, and tool calling
---
import { Steps } from '@astrojs/starlight/components';

The AI module provides a clean, maintainable service for AI text generation using the Vercel AI SDK with Langfuse telemetry, model flexibility, cancellation support, optional Zod schema validation, and tool calling support.

## Features

- **Multiple Model Support**: Runtime model selection with pre-registered models
- **Multiple Provider Support**: OpenAI, Google (Gemini), Anthropic (Claude), and Mistral
- **Langfuse Integration**: Automatic telemetry and tracing with full prompt visibility
- **Cancellation Support**: Both AbortSignal parameter and returned AbortController
- **Schema Validation**: Optional Zod schema validation via prompt commands with discriminated union types
- **Tool Calling**: Support for AI tool/function calling with automatic result handling
- **Type Safety**: Full TypeScript support with inferred types and function overloads

## Configuration

<Steps>

1. ### Add environment variables

   Add the following environment variables to your `.env` file:

   ```env
   # Langfuse (optional but recommended)
   LANGFUSE_SECRET_KEY=your-secret-key
   LANGFUSE_PUBLIC_KEY=your-public-key  # Optional
   LANGFUSE_HOST=https://cloud.langfuse.com  # Optional, defaults to cloud

   # Provider API Keys (set the ones you want to use)
   OPENAI_API_KEY=your-openai-api-key
   ANTHROPIC_API_KEY=your-anthropic-api-key  # If using Anthropic
   GOOGLE_API_KEY=your-google-api-key  # If using Google
   MISTRAL_API_KEY=your-mistral-api-key  # If using Mistral
   ```

   :::tip[Langfuse Setup]
   Langfuse is optional but highly recommended for prompt management, versioning, and debugging. You can self-host Langfuse or use their cloud service. See the [Langfuse documentation](https://langfuse.com/docs) for setup instructions.
   :::

   <br/>

2. ### Verify module is registered

   The AI module should already be imported in your `app.module.ts`. Verify it's included:

   ```typescript title="src/app.module.ts"
   import { AiModule } from './modules/ai/ai.module'

   @Module({
     imports: [
       // ... other modules
       AiModule,
     ],
   })
   export class AppModule {}
   ```

   <br/>

3. ### Check available models

   Models are pre-configured in `ai.config.ts`. Available models:
   - `OPENAI_GPT_5_NANO`: GPT-5 Nano
   - `GOOGLE_GEMINI_3_FLASH`: Gemini 3 Flash
   - `CLAUDE_HAIKU_3_5`: Claude 3.5 Haiku
   - `MISTRAL_SMALL`: Mistral Small

   :::note[No Default Model]
   No default model is set. You must specify a model when calling `generate()`.
   :::

</Steps>

## Usage

### Basic Text Generation

Inject `AiService` into your service or controller and use it for text generation:

```typescript
import { Injectable } from '@nestjs/common'
import { AiService } from '../modules/ai/ai.service'
import { GOOGLE_GEMINI_3_FLASH } from '../modules/ai/ai.config'

@Injectable()
export class MyService {
  constructor(private readonly aiService: AiService) {}

  async generateContent() {
    const result = await this.aiService.generate({
      prompt: 'Write a short story about a robot',
      model: GOOGLE_GEMINI_3_FLASH,
    })

    // TypeScript knows result.type === 'text'
    if (result.type === 'text') {
      console.log(result.result) // string
      console.log(result.usage) // Token usage information
    }
  }
}
```

### With Schema Validation

Use Zod schemas to get structured, validated output:

```typescript
import { z } from 'zod'
import { GOOGLE_GEMINI_3_FLASH } from '../modules/ai/ai.config'

const userProfileSchema = z.object({
  name: z.string(),
  age: z.number(),
  email: z.string().email(),
})

async function generateUserProfile() {
  // TypeScript infers the return type based on the schema
  const result = await this.aiService.generate({
    prompt: 'Generate a user profile',
    model: GOOGLE_GEMINI_3_FLASH,
    schema: userProfileSchema,
  })

  // TypeScript knows result.type === 'object' and result.result is typed as the schema
  if (result.type === 'object') {
    // result.result is typed as { name: string; age: number; email: string }
    console.log(result.result.name)
    console.log(result.result.age)
    console.log(result.result.email)
  }
}
```

### With Cancellation

Cancel long-running generations using AbortSignal:

```typescript
import { GOOGLE_GEMINI_3_FLASH } from '../modules/ai/ai.config'

const abortController = new AbortController()

const promise = this.aiService.generate({
  prompt: 'Long running task',
  model: GOOGLE_GEMINI_3_FLASH,
  signal: abortController.signal,
})

// Cancel after 5 seconds
setTimeout(() => {
  abortController.abort()
}, 5000)

try {
  const result = await promise
} catch (error) {
  if (error instanceof Error && error.name === 'AbortError') {
    console.log('Generation was cancelled')
  }
}
```

Alternatively, use the returned AbortController:

```typescript
const result = await this.aiService.generate({
  prompt: 'Long running task',
  model: GOOGLE_GEMINI_3_FLASH,
})

// Cancel later if needed
if (result.abortController) {
  setTimeout(() => {
    result.abortController?.abort()
  }, 5000)
}
```

### With Generation Options

Customize generation behavior with options:

```typescript
import { GOOGLE_GEMINI_3_FLASH } from '../modules/ai/ai.config'

const result = await this.aiService.generate({
  prompt: 'Generate creative content',
  model: GOOGLE_GEMINI_3_FLASH,
  options: {
    temperature: 0.9, // Higher creativity
    maxTokens: 500,
    topP: 0.95,
    frequencyPenalty: 0.5,
    presencePenalty: 0.3,
  },
})
```

### With Tool Calling

Tools allow the AI to call functions during generation. Create tools using the `tool()` function from the `ai` SDK:

```typescript
import { GOOGLE_GEMINI_3_FLASH } from '../modules/ai/ai.config'
import { tool } from 'ai'
import { z } from 'zod'

const getWeatherTool = tool({
  description: 'Get the current weather for a location',
  inputSchema: z.object({
    location: z.string().describe('The city and state, e.g. San Francisco, CA'),
    unit: z.enum(['celsius', 'fahrenheit']).optional().describe('Temperature unit'),
  }),
  outputSchema: z.object({
    temperature: z.number(),
    condition: z.string(),
    humidity: z.number(),
  }),
  execute: async ({ location, unit = 'celsius' }) => {
    // Your tool implementation
    const weather = await fetchWeatherAPI(location, unit)
    return {
      temperature: weather.temp,
      condition: weather.condition,
      humidity: weather.humidity,
    }
  },
})

const result = await this.aiService.generate({
  prompt: 'What is the weather in San Francisco?',
  model: GOOGLE_GEMINI_3_FLASH,
  tools: {
    getWeather: getWeatherTool,
  },
})

// Check for tool calls
if (result.toolCalls && result.toolCalls.length > 0) {
  console.log('Tool calls:', result.toolCalls)
  // Each tool call has: toolCallId, toolName, args
}

// Check for tool results
if (result.toolResults && result.toolResults.length > 0) {
  console.log('Tool results:', result.toolResults)
  // Each tool result has: toolCallId, toolName, result
}
```

### Using MCP (Model Context Protocol) Tools

You can also use tools from MCP servers. See the example in `tools/coingecko.tools.ts`:

```typescript
import { GOOGLE_GEMINI_3_FLASH } from '../modules/ai/ai.config'
import { createCoingeckoMCPClient, getCryptoPriceTool } from '../modules/ai/tools/coingecko.tools'

// Create MCP client
const coingeckoMCPClient = await createCoingeckoMCPClient()
const mcpTools = await coingeckoMCPClient.tools()

// Combine MCP tools with custom tools
const tools = {
  ...mcpTools, // Spread MCP tools
  getCryptoPrice: getCryptoPriceTool, // Add custom tool
}

const result = await this.aiService.generate({
  prompt: 'What is the current price of Bitcoin?',
  model: GOOGLE_GEMINI_3_FLASH,
  tools,
})
```

## Advanced Topics

### Adding New Models

To add a new model, edit `apps/api/src/modules/ai/ai.config.ts`:

```typescript title="src/modules/ai/ai.config.ts"
export const modelConfigBase = {
  // ... existing models
  OPENAI_GPT_4_TURBO: {
    modelString: 'gpt-4-turbo',
    provider: 'openai' as ProviderName,
    isDefault: false,
  },
  GOOGLE_GEMINI_PRO: {
    modelString: 'gemini-pro',
    provider: 'google' as ProviderName,
    isDefault: false,
  },
} as const satisfies Record<string, ModelConfig>
```

**Steps:**
1. Add a new entry to `modelConfigBase` with:
   - A unique constant name (e.g., `OPENAI_GPT_4_TURBO`)
   - `modelString`: The actual model identifier used by the provider
   - `provider`: One of `'openai'`, `'google'`, `'anthropic'`, or `'mistral'`
   - `isDefault`: Set to `true` if this should be the default model
2. The model will be automatically registered when the module initializes
3. Use it by importing the constant: `import { OPENAI_GPT_4_TURBO } from '../modules/ai/ai.config'`

### Creating Custom Tools

Tools are created using the `tool()` function from the `ai` SDK. Create tools in the `tools/` directory:

```typescript title="src/modules/ai/tools/my-tool.ts"
import { tool } from 'ai'
import { z } from 'zod'

const myToolInputSchema = z.object({
  param1: z.string().describe('Description of param1'),
  param2: z.number().optional().describe('Description of param2'),
})

export const myTool = tool({
  description: 'What this tool does',
  inputSchema: myToolInputSchema,
  outputSchema: z.object({
    // Define your output schema
    result: z.string(),
  }),
  execute: async ({ param1, param2 }) => {
    // Your tool implementation
    // This function is called when the AI decides to use this tool
    return {
      result: 'Tool output',
    }
  },
})
```

### Adding New Providers

To add support for a new provider (e.g., Cohere):

1. **Install the Provider Package**

   ```bash
   cd apps/api
   pnpm add @ai-sdk/cohere
   ```

2. **Add Provider Creation Function**

   Edit `apps/api/src/modules/ai/ai.providers.ts`:

   ```typescript title="src/modules/ai/ai.providers.ts"
   async function loadCohereProvider(apiKey: string): Promise<ProviderInstance> {
     if (!cohereProviderCache.has(apiKey)) {
       try {
         const { createCohere } = await import('@ai-sdk/cohere')
         const provider = createCohere({ apiKey })
         cohereProviderCache.set(apiKey, provider)
       }
       catch (error) {
         if (error instanceof Error && (error.message.includes('Cannot find module') || error.message.includes('Failed to resolve'))) {
           throw new Error('Cohere provider requires @ai-sdk/cohere package. Please install it: pnpm add @ai-sdk/cohere')
         }
         throw error
       }
     }
     return cohereProviderCache.get(apiKey)!
   }

   export async function createCohere({ apiKey }: { apiKey?: string }): Promise<ProviderInstance> {
     if (!apiKey) {
       throw new Error('COHERE_API_KEY is required for Cohere provider')
     }
     return loadCohereProvider(apiKey)
   }
   ```

   Don't forget to add the cache:
   ```typescript
   const cohereProviderCache: Map<string, ProviderInstance> = new Map()
   ```

3. **Add Environment Variable**

   Edit `apps/api/src/config/env.config.ts`:

   ```typescript title="src/config/env.config.ts"
   export const configValidationSchema = z.object({
     // ... existing variables
     COHERE_API_KEY: z.string().optional(),
   })

   export const config = {
     // ... existing config
     ai: {
       providers: {
         // ... existing providers
         cohere: {
           apiKey: configParsed.data.COHERE_API_KEY,
         },
       },
     },
   }
   ```

4. **Register Provider in Config**

   Edit `apps/api/src/modules/ai/ai.config.ts`:

   ```typescript title="src/modules/ai/ai.config.ts"
   import { createCohere } from './ai.providers'

   export const providers: {
     openai: ProviderInstance | null
     google: Promise<ProviderInstance> | null
     anthropic: Promise<ProviderInstance> | null
     cohere: Promise<ProviderInstance> | null  // Add new provider
   } = {
     // ... existing providers
     cohere: config.ai.providers.cohere.apiKey ? createCohere({ apiKey: config.ai.providers.cohere.apiKey }) : null,
   }
   ```

5. **Add Models Using the New Provider**

   Now you can add models that use the new provider:

   ```typescript
   export const modelConfigBase = {
     // ... existing models
     COHERE_COMMAND_R: {
       modelString: 'command-r',
       provider: 'cohere' as ProviderName,
       isDefault: false,
     },
   } as const
   ```

## REST API Endpoint

The module includes a REST API controller at `/ai/chat` for easy integration:

```typescript
POST /ai/chat
Content-Type: application/json

{
  "message": "What is the weather in San Francisco?",
  "model": "GOOGLE_GEMINI_3_FLASH", // Optional
  "options": { // Optional
    "temperature": 0.7,
    "maxTokens": 500
  },
  "schemaType": "userProfile" // Optional: 'userProfile', 'task', 'product', or 'none'
}
```

:::note[Authentication Required]
The `/ai/chat` endpoint requires authentication. Make sure to include your authentication token in the request headers.
:::

## Langfuse Integration

All AI calls are automatically traced in Langfuse (if configured) with:
- Trace name: `ai.generate`
- **Full prompt**: Both original and enhanced (with schema commands) prompts are included
- Input: Complete enhanced prompt sent to the model
- Metadata: model ID, model name, prompt lengths, schema presence, tool presence, options
- Token usage and costs
- Latency metrics
- Tool calls and results
- Error tracking

View your traces in the Langfuse dashboard to debug issues, optimize prompts, and monitor costs.

## Type Safety

The service uses TypeScript function overloads for type safety:

- When no schema is provided: `result.type === 'text'` and `result.result` is `string`
- When a schema is provided: `result.type === 'object'` and `result.result` matches the schema type

TypeScript will automatically narrow the type based on the discriminated union:

```typescript
const result = await this.aiService.generate({
  prompt: 'Generate content',
  model: GOOGLE_GEMINI_3_FLASH,
})

// TypeScript narrows the type based on the discriminated union
if (result.type === 'text') {
  // result.result is string
  console.log(result.result)
} else if (result.type === 'object') {
  // result.result matches the schema type
  console.log(result.result)
}
```

## Troubleshooting

### Provider Not Available

If you get an error that a provider is not available:
1. Check that the API key is set in your `.env` file
2. Verify the API key is correct
3. Ensure the provider package is installed: `pnpm add @ai-sdk/[provider]`

### Model Not Found

If you get a "model is not registered" error:
1. Check that the model exists in `modelConfigBase`
2. Verify the model ID constant is imported correctly
3. Ensure the provider for that model has a configured API key

### Schema Validation Fails

If schema validation fails:
1. Check that the prompt includes clear instructions about JSON format
2. Verify the Zod schema matches the expected structure
3. Check Langfuse traces to see what the model actually returned

### Langfuse Not Working

If Langfuse traces aren't appearing:
1. Verify `LANGFUSE_SECRET_KEY` is set in your `.env` file
2. Check that the Langfuse client initialized (check logs for "Langfuse client initialized")
3. Ensure your Langfuse project is set up correctly
4. Check network connectivity to Langfuse (default: `https://cloud.langfuse.com`)

## Testing

When testing services that use `AiService`, you can mock it:

```typescript
// Mock for text generation
const mockAiService = {
  generate: jest.fn().mockResolvedValue({
    type: 'text' as const,
    result: 'Mocked response',
    usage: { promptTokens: 10, completionTokens: 20, totalTokens: 30 },
  }),
}

// Mock for object generation with schema
const mockAiServiceWithSchema = {
  generate: jest.fn().mockResolvedValue({
    type: 'object' as const,
    result: { name: 'John', age: 30 },
    usage: { promptTokens: 10, completionTokens: 20, totalTokens: 30 },
  }),
}
```

## Architecture

The module is organized into several files:

- **`ai.config.ts`**: Provider and model configuration
- **`ai.providers.ts`**: Provider creation functions using SDK `create*` methods
- **`ai.utils.ts`**: Helper functions for model retrieval and JSON sanitization
- **`ai.service.ts`**: Main service with `generate` method and function overloads
- **`contracts/ai.contract.ts`**: Zod schemas and discriminated union types for type safety
- **`ai.controller.ts`**: REST API controller for chat endpoint
- **`tools/`**: Tool definitions for AI function calling

## Going Further

- Explore the [Vercel AI SDK documentation](https://sdk.vercel.ai/docs) for advanced features
- Check out [Langfuse documentation](https://langfuse.com/docs) for prompt management and analytics
- See the example tool implementation in `tools/coingecko.tools.ts` for reference
- Review the [AI explanation](/lonestone-boilerplate/explanations/core-features/ai) for architectural details
